{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we train a model using more of Cellposes' pre- and post-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from import_images import getImages\n",
    "import numpy as np\n",
    "import torch\n",
    "from cellpose import resnet_torch\n",
    "from cellpose import transforms\n",
    "import cv2\n",
    "import time\n",
    "from unet_architecture import UNet\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchmetrics.classification import BinaryJaccardIndex\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import ezomero\n",
    "from omero_data import connect, extract_channel, progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pre_activations(image,cpnet):\n",
    "    x = torch.from_numpy(image)\n",
    "    downsample = cpnet.downsample(x)\n",
    "    style = cpnet.make_style(downsample[-1])\n",
    "    upsample = cpnet.upsample(style, downsample, cpnet.mkldnn)\n",
    "    \n",
    "    output = cpnet.output(upsample).squeeze(0)\n",
    "    output = output.cpu().detach().numpy()\n",
    "    output = np.array(output)\n",
    "\n",
    "    upsample = upsample.squeeze(0)\n",
    "    upsample = upsample.cpu().detach().numpy().tolist()\n",
    "    upsample = np.array(upsample)\n",
    "    return upsample, output\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image, upsample, cellprob):\n",
    "        self.image = image\n",
    "        self.upsample = upsample\n",
    "        self.cellprob = cellprob\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.image[idx]\n",
    "        upsample = self.upsample[idx]\n",
    "        cellprob = self.cellprob[idx]\n",
    "        return img, upsample, cellprob\n",
    "    \n",
    "class KD_loss(torch.nn.Module):\n",
    "    def __init__(self, alpha, beta):\n",
    "        super(KD_loss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def forward(self, y_32_pred, y_32_true, y_3_pred, y_3_true):\n",
    "        #32-channel loss\n",
    "        y_32_pred = F.sigmoid(y_32_pred)\n",
    "        y_32_true = F.sigmoid(y_32_true)\n",
    "        y_32_loss = F.mse_loss(y_32_pred, y_32_true.float())\n",
    "        #3-channel loss\n",
    "        flow_loss = F.mse_loss(y_3_pred[:,:2], y_3_true[:,:2])\n",
    "        flow_loss /= 2.\n",
    "        map_loss = F.mse_loss(y_3_pred[:,2] , y_3_true[:,2])\n",
    "        y_3_loss = flow_loss + map_loss\n",
    "        return y_32_loss * self.alpha, y_3_loss * self.beta\n",
    "\n",
    "def trainEpoch(unet, train_loader, test_loader, validation_loader, loss_fn, optimiser, scheduler, epoch_num, device):\n",
    "    time_start = time.time()\n",
    "    \n",
    "    unet.train()\n",
    "\n",
    "    train_y_32_loss, train_map_loss = 0, 0\n",
    "\n",
    "    for image, upsample, cp_output in train_loader:\n",
    "\n",
    "        \n",
    "        image, upsample, cp_output = image.float(), upsample.float(), cp_output.float() #cast to float32 (important for mps)\n",
    "\n",
    "        if device is not None:\n",
    "            (image, upsample, cp_output) = (image.to(device),upsample.to(device),cp_output.to(device)) # sending the data to the device (cpu or GPU)\n",
    "\n",
    "        #image = image.unsqueeze(1)\n",
    "        y_16_pred, y_32_pred, map_pred = unet(image)\n",
    "        y_32_pred = y_32_pred.squeeze(1)\n",
    "        map_pred = map_pred.squeeze(1)\n",
    "    \n",
    "        loss_32, loss_map = loss_fn(y_32_pred,  upsample, map_pred, cp_output) # calculate the loss of that prediction\n",
    "        train_y_32_loss += loss_32.item()\n",
    "        train_map_loss += loss_map.item()\n",
    "\n",
    "         # zero out the accumulated gradients\n",
    "\n",
    "        #I want to get two losses, one for the 32-channel output and one for the 3-channel output\n",
    "        #I then want to freeze certain channels before putting the losses backwards\n",
    "        unet.encoder.requires_grad = True #repetitive but just to be clear\n",
    "        unet.decoder.requires_grad = True\n",
    "        unet.head.requires_grad = False\n",
    "        loss_32.backward(retain_graph=True)\n",
    "\n",
    "        \n",
    "        unet.encoder.requires_grad = False\n",
    "        unet.decoder.requires_grad = False\n",
    "        unet.head.requires_grad = True\n",
    "        loss_map.backward(retain_graph=True)\n",
    "\n",
    "        optimiser.step() # update model parameters\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    train_y_32_loss, train_map_loss = train_y_32_loss/len(train_loader), train_map_loss/len(train_loader)\n",
    "\n",
    "\n",
    "    val_y_32_loss, val_map_loss, val_IoU = 0, 0, 0\n",
    "    for image, upsample, cp_output in validation_loader:\n",
    "        \n",
    "        image, upsample, cp_output = image.float(), upsample.float(), cp_output.float() #cast to float32 (important for mps)\n",
    "\n",
    "        if device is not None:\n",
    "            (image, upsample, cp_output) = (image.to(device),upsample.to(device),cp_output.to(device)) # sending the data to the device (cpu or GPU)\n",
    "\n",
    "        #image = image.unsqueeze(1)\n",
    "        y_16_pred, y_32_pred, map_pred = unet(image)\n",
    "        y_32_pred = y_32_pred.squeeze(1)\n",
    "        map_pred = map_pred.squeeze(1)\n",
    "    \n",
    "        loss_32, loss_map = loss_fn(y_32_pred,  upsample, map_pred, cp_output) # calculate the loss of that prediction\n",
    "        val_y_32_loss += loss_32.item()\n",
    "        val_map_loss += loss_map.item()\n",
    "\n",
    "        #IoU score\n",
    "        jaccard = BinaryJaccardIndex(threshold=0.5).to(device)\n",
    "        map_pred = F.sigmoid(map_pred)\n",
    "        cp_output = F.sigmoid(cp_output)\n",
    "        cp_output = torch.where(cp_output > 0.5, 1.0, 0.0)\n",
    "        iou = jaccard(map_pred, cp_output)\n",
    "        if not torch.isnan(iou):\n",
    "            val_IoU += iou\n",
    "        else:\n",
    "            val_IoU += 0\n",
    "        \n",
    "\n",
    "    val_y_32_loss, val_map_loss, val_IoU = val_y_32_loss/len(validation_loader), val_map_loss/len(validation_loader), val_IoU.item()/len(validation_loader)\n",
    "    \n",
    "    #we might add displaying later on\n",
    "    \n",
    "    if epoch_num is None:\n",
    "        print('Train 32 loss: ', train_y_32_loss,'Train map loss', train_map_loss, 'Val 32 loss: ', val_y_32_loss, 'Val map loss: ', val_map_loss, 'Val IoU: ', val_IoU, 'Time: ', time.time()-time_start)\n",
    "    else:\n",
    "        print('Epoch: ', epoch_num, 'Train 32 loss: ', train_y_32_loss,'Train map loss', train_map_loss, 'Val 32 loss: ', val_y_32_loss, 'Val map loss: ', val_map_loss, 'Val IoU: ', val_IoU, 'Time: ', time.time()-time_start)\n",
    "\n",
    "    return unet    \n",
    "\n",
    "def get_omero_images_combined(num_images=None, channels=[0,0]):\n",
    "    conn = connect(user='rz200',password='omeroreset')\n",
    "\n",
    "    plate = 1237\n",
    "    if num_images == None:\n",
    "        image_ids = ezomero.get_image_ids(conn,plate=plate)\n",
    "    else:\n",
    "        image_ids = ezomero.get_image_ids(conn,plate=plate)[:num_images]\n",
    "\n",
    "    print('In plate',plate,'we have',len(image_ids),'images')\n",
    "\n",
    "   \n",
    "\n",
    "    data_images = []\n",
    "    for i in progressbar(range(len(image_ids)), \"Computing: \", 40):\n",
    "        data_images.append(ezomero.get_image(conn, image_ids[i])[1])\n",
    "\n",
    "    if len(channels) == 2 and channels != [0,0]:\n",
    "        data_images_one = extract_channel(data_images, channels[0])\n",
    "        data_images_two = extract_channel(data_images, channels[1])\n",
    "\n",
    "        combined_images = []\n",
    "        for i in range(len(data_images_one)):\n",
    "            combined_images.append(np.array([data_images_one[i] , data_images_two[i]]))\n",
    "    \n",
    "        return combined_images\n",
    "    elif len(channels) == 1:\n",
    "        data_images_one = extract_channel(data_images, channels[0])\n",
    "        return data_images_one\n",
    "    else:\n",
    "        return data_images\n",
    "\n",
    "def get_cellpose_data(cpnet, combined_images):\n",
    "    images_tiled = []\n",
    "    ys = []\n",
    "    all_upsamples = []\n",
    "\n",
    "    for i in range(len(combined_images)):\n",
    "        print(i)\n",
    "        image_t = combined_images[i]\n",
    "\n",
    "        if len(image_t.shape) == 2:\n",
    "            image_t = [image_t,image_t]\n",
    "            image_t = np.array(image_t)\n",
    "\n",
    "        IMG, ysub, xsub, Ly, Lx = transforms.make_tiles(image_t, bsize=224, \n",
    "                                                        augment=False, tile_overlap=0.1)\n",
    "        ny, nx, nchan, ly, lx = IMG.shape\n",
    "        IMG = np.reshape(IMG, (ny*nx, nchan, ly, lx))\n",
    "        images_tiled.append(IMG)\n",
    "        batch_size = 1\n",
    "        niter = int(np.ceil(IMG.shape[0] / batch_size))\n",
    "        nout = 3 + 32*False\n",
    "        y = np.zeros((IMG.shape[0], nout, ly, lx))\n",
    "        styles = []\n",
    "\n",
    "        upsamples = []\n",
    "\n",
    "        for k in range(niter):\n",
    "            irange = np.arange(batch_size*k, min(IMG.shape[0], batch_size*k+batch_size))\n",
    "            upsample, y0 = get_pre_activations(IMG[irange], cpnet)\n",
    "            upsamples.append(upsample)\n",
    "            y[irange] = y0.reshape(len(irange), y0.shape[-3], y0.shape[-2], y0.shape[-1])\n",
    "\n",
    "        ys.append(y)\n",
    "\n",
    "        upsamples = np.array(upsamples)\n",
    "        all_upsamples.append(upsamples)\n",
    "\n",
    "    images_tiled_np = np.array(images_tiled)\n",
    "    ys_np = np.array(ys)\n",
    "    all_upsamples_np = np.array(all_upsamples)\n",
    "\n",
    "    images_tiled_np = images_tiled_np.reshape(images_tiled_np.shape[0]*images_tiled_np.shape[1], images_tiled_np.shape[2], images_tiled_np.shape[3], images_tiled_np.shape[4])\n",
    "    ys_np = ys_np.reshape(ys_np.shape[0]*ys_np.shape[1], ys_np.shape[2], ys_np.shape[3], ys_np.shape[4])\n",
    "    all_upsamples_np = all_upsamples_np.reshape(all_upsamples_np.shape[0]*all_upsamples_np.shape[1], all_upsamples_np.shape[2], all_upsamples_np.shape[3], all_upsamples_np.shape[4])\n",
    "        \n",
    "    return images_tiled_np, ys_np, all_upsamples_np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"/Users/rz200/Documents/development/distillCellSegTrack/pipeline/CellPose_models/U2OS_Tub_Hoechst\"\n",
    "cpnet = resnet_torch.CPnet(nbase=[2,32,64,128,256],nout=3,sz=3)\n",
    "cpnet.load_model(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful\n",
      "In plate 1237 we have 2 images\n",
      "Computing: [########################################] 2/2\n",
      "\n",
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m combined_images \u001b[39m=\u001b[39m get_omero_images_combined(num_images\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, channels\u001b[39m=\u001b[39m[\u001b[39m0\u001b[39m])\n\u001b[1;32m----> 2\u001b[0m images_tiled, ys, all_upsamples \u001b[39m=\u001b[39m get_cellpose_data(cpnet, combined_images)\n",
      "Cell \u001b[1;32mIn[12], line 182\u001b[0m, in \u001b[0;36mget_cellpose_data\u001b[1;34m(cpnet, combined_images)\u001b[0m\n\u001b[0;32m    179\u001b[0m     image_t \u001b[39m=\u001b[39m [image_t,image_t]\n\u001b[0;32m    180\u001b[0m     image_t \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(image_t)\n\u001b[1;32m--> 182\u001b[0m IMG, ysub, xsub, Ly, Lx \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39;49mmake_tiles(image_t, bsize\u001b[39m=\u001b[39;49m\u001b[39m224\u001b[39;49m, \n\u001b[0;32m    183\u001b[0m                                                 augment\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, tile_overlap\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m)\n\u001b[0;32m    184\u001b[0m ny, nx, nchan, ly, lx \u001b[39m=\u001b[39m IMG\u001b[39m.\u001b[39mshape\n\u001b[0;32m    185\u001b[0m IMG \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(IMG, (ny\u001b[39m*\u001b[39mnx, nchan, ly, lx))\n",
      "File \u001b[1;32mc:\\Users\\rz200\\Anaconda3\\envs\\cellpose\\lib\\site-packages\\cellpose\\transforms.py:133\u001b[0m, in \u001b[0;36mmake_tiles\u001b[1;34m(imgi, bsize, augment, tile_overlap)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_tiles\u001b[39m(imgi, bsize\u001b[39m=\u001b[39m\u001b[39m224\u001b[39m, augment\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, tile_overlap\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" make tiles of image to run at test-time\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \n\u001b[0;32m     99\u001b[0m \u001b[39m    if augmented, tiles are flipped and tile_overlap=2.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[39m    \u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m     nchan, Ly, Lx \u001b[39m=\u001b[39m imgi\u001b[39m.\u001b[39mshape\n\u001b[0;32m    134\u001b[0m     \u001b[39mif\u001b[39;00m augment:\n\u001b[0;32m    135\u001b[0m         bsize \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mint32(bsize)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "combined_images = get_omero_images_combined(num_images=2, channels=[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1080, 1080)\n"
     ]
    }
   ],
   "source": [
    "print(combined_images[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "images_tiled, ys, all_upsamples = get_cellpose_data(cpnet, combined_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72, 2, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "print(images_tiled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, test_images, train_upsamples, test_upsamples, train_cellprob, test_cellprob = train_test_split(images_tiled[:100], all_upsamples[:100], ys[:100], test_size=0.1, random_state=42)\n",
    "train_images, val_images, train_upsamples, val_upsamples, train_cellprob, val_cellprob = train_test_split(train_images, train_upsamples, train_cellprob, test_size=0.1, random_state=42)\n",
    "\n",
    "num_train_images = 10\n",
    "train_dataset = ImageDataset(train_images, train_upsamples, train_cellprob)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "validation_dataset = ImageDataset(val_images, val_upsamples, val_cellprob)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "test_dataset = ImageDataset(test_images, test_upsamples, test_cellprob)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet(encChannels=(2,32,64,128,256),decChannels=(256,128,64,32),nbClasses=3) #it's not a problem to train it with 2 channels as anyway it is just the same channel repeated twice for the nuclei model but that may make more parameters to train so we may want to chang ethat\n",
    "unet = unet.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rz200\\Anaconda3\\envs\\cellpose\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Train 32 loss:  0.2979394495487213 Train map loss 12.23060953617096 Val 32 loss:  0.2917259633541107 Val map loss:  9.416550636291504 Val IoU:  0.3035811483860016 Time:  6.260850429534912\n",
      "Epoch:  1 Train 32 loss:  0.2890694625675678 Train map loss 8.242403268814087 Val 32 loss:  0.29161548614501953 Val map loss:  7.107572078704834 Val IoU:  0.1025366485118866 Time:  5.551206111907959\n",
      "Epoch:  2 Train 32 loss:  0.2872977592051029 Train map loss 6.266357958316803 Val 32 loss:  0.2915608584880829 Val map loss:  6.081027984619141 Val IoU:  0.09498772770166397 Time:  5.622990846633911\n",
      "Epoch:  3 Train 32 loss:  0.2941582016646862 Train map loss 6.587083220481873 Val 32 loss:  0.2837829887866974 Val map loss:  5.528446197509766 Val IoU:  0.10075341165065765 Time:  5.659926414489746\n",
      "Epoch:  4 Train 32 loss:  0.3036434054374695 Train map loss 10.10155564546585 Val 32 loss:  0.28052452206611633 Val map loss:  6.998315334320068 Val IoU:  0.07141703367233276 Time:  5.698857069015503\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m scheduler \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mStepLR(optimiser, step_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, gamma\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m500\u001b[39m):\n\u001b[0;32m      8\u001b[0m     \u001b[39m#print(scheduler.get_last_lr())\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     unet \u001b[39m=\u001b[39m trainEpoch(unet, train_loader, test_loader, validation_loader, loss_fn, optimiser, scheduler\u001b[39m=\u001b[39;49mscheduler, epoch_num\u001b[39m=\u001b[39;49mepoch, device\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcuda:0\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[42], line 62\u001b[0m, in \u001b[0;36mtrainEpoch\u001b[1;34m(unet, train_loader, test_loader, validation_loader, loss_fn, optimiser, scheduler, epoch_num, device)\u001b[0m\n\u001b[0;32m     59\u001b[0m image, upsample, cp_output \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mfloat(), upsample\u001b[39m.\u001b[39mfloat(), cp_output\u001b[39m.\u001b[39mfloat() \u001b[39m#cast to float32 (important for mps)\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 62\u001b[0m     (image, upsample, cp_output) \u001b[39m=\u001b[39m (image\u001b[39m.\u001b[39;49mto(device),upsample\u001b[39m.\u001b[39mto(device),cp_output\u001b[39m.\u001b[39mto(device)) \u001b[39m# sending the data to the device (cpu or GPU)\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39m#image = image.unsqueeze(1)\u001b[39;00m\n\u001b[0;32m     65\u001b[0m y_16_pred, y_32_pred, map_pred \u001b[39m=\u001b[39m unet(image)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_fn = KD_loss(alpha=1, beta=1)\n",
    "optimiser = torch.optim.SGD(unet.parameters(), lr=0.01, momentum=0.1)\n",
    "\n",
    "#I think a decaying scheduler is best, not a cyclic one\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=100, gamma=0.1)\n",
    "\n",
    "for epoch in range(500):\n",
    "    #print(scheduler.get_last_lr())\n",
    "    unet = trainEpoch(unet, train_loader, test_loader, validation_loader, loss_fn, optimiser, scheduler=scheduler, epoch_num=epoch, device='cuda:0')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellprob2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
