{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rz200\\AppData\\Local\\anaconda3\\envs\\cellprob\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from import_images import getImages\n",
    "import numpy as np\n",
    "import torch\n",
    "from cellpose import resnet_torch\n",
    "from cellpose import transforms\n",
    "import cv2\n",
    "import time\n",
    "from unet_architecture import UNet\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchmetrics.classification import BinaryJaccardIndex\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pre_activations(image,cpnet):\n",
    "\n",
    "    x = transforms.resize_image(image, rsz=cpnet.diam_mean/cpnet.diam_labels,no_channels=True).astype(np.float32)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = np.concatenate((x, x), axis=0)\n",
    "    x = torch.from_numpy(x).unsqueeze(0)\n",
    "\n",
    "    downsample = cpnet.downsample(x)\n",
    "    style = cpnet.make_style(downsample[-1])\n",
    "    upsample = cpnet.upsample(style, downsample, cpnet.mkldnn)\n",
    "\n",
    "    output = cpnet.output(upsample).squeeze(0)[2]\n",
    "    output = output.cpu().detach().numpy()\n",
    "    output = cv2.resize(output, dsize=(1024, 1024), interpolation=cv2.INTER_NEAREST)\n",
    "    output = np.array(output)\n",
    "\n",
    "    upsample = upsample.squeeze(0)\n",
    "    upsample = upsample.cpu().detach().numpy().tolist()\n",
    "    for (k, image) in enumerate(upsample):\n",
    "        upsample[k] = cv2.resize(np.array(image), dsize=(1024, 1024), interpolation=cv2.INTER_NEAREST)\n",
    "    upsample = np.array(upsample)\n",
    "\n",
    "    return upsample, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image, upsample, cellprob):\n",
    "        self.image = image\n",
    "        self.upsample = upsample\n",
    "        self.cellprob = cellprob\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.image[idx]\n",
    "        upsample = self.upsample[idx]\n",
    "        cellprob = self.cellprob[idx]\n",
    "        return img, upsample, cellprob\n",
    "    \n",
    "def get_crops(image,size):\n",
    "    assert image.shape[0]%size == 0\n",
    "    assert image.shape[0] == image.shape[1]\n",
    "    crops = []\n",
    "    for i in range(0,image.shape[0],size):\n",
    "        for j in range(0,image.shape[1],size):\n",
    "            crops.append(image[i:i+size,j:j+size])\n",
    "    return crops\n",
    "\n",
    "def reconstruct_image(crops, size):\n",
    "    assert len(crops) > 0\n",
    "    image_size = 1024\n",
    "    size = 256\n",
    "    image = np.zeros((image_size, image_size), dtype=np.uint8)\n",
    "    crop_idx = 0\n",
    "    for i in range(0, image_size, size):\n",
    "        for j in range(0, image_size, size):\n",
    "            image[i:i+size, j:j+size] = crops[crop_idx]\n",
    "            crop_idx += 1\n",
    "    return image\n",
    "\n",
    "def get_data(images, cpnet):\n",
    "    cp_upsamples = []\n",
    "    cp_outputs = []\n",
    "    for image in images:\n",
    "        upsample, output = get_pre_activations(image,cpnet)\n",
    "        cp_upsamples.append(upsample)\n",
    "        cp_outputs.append(output)\n",
    "\n",
    "    images_torch = torch.from_numpy(np.array(images))\n",
    "    train_images, test_images, train_upsamples, test_upsamples, train_cellprob, test_cellprob = train_test_split(images_torch, cp_upsamples, cp_outputs, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "    images_normalised = []\n",
    "    for image in train_images:\n",
    "        min_val = torch.min(image)\n",
    "        max_val = torch.max(image)\n",
    "        normalized_image = (image - min_val) / (max_val - min_val)\n",
    "        images_normalised.append(normalized_image)\n",
    "\n",
    "    images_torch_rotated = []\n",
    "    for image in images_normalised:\n",
    "        images_torch_rotated.append(image)\n",
    "        for i in range(3):\n",
    "            image = torch.rot90(image, 1, [0, 1])\n",
    "            images_torch_rotated.append(image)\n",
    "\n",
    "    images_torch_rotated_flipped = []\n",
    "    for image in images_torch_rotated:\n",
    "        images_torch_rotated_flipped.append(image)\n",
    "        images_torch_rotated_flipped.append(torch.flip(image, [1]))\n",
    "\n",
    "    cp_upsamples_normalised = []\n",
    "    for upsample in train_upsamples:\n",
    "        min_val = np.min(upsample)\n",
    "        max_val = np.max(upsample)\n",
    "        normalized_upsample = (upsample - min_val) / (max_val - min_val)\n",
    "        cp_upsamples_normalised.append(normalized_upsample)\n",
    "\n",
    "    cp_upsamples_rotated = []\n",
    "    for upsample in cp_upsamples_normalised:\n",
    "        cp_upsamples_rotated.append(upsample)\n",
    "        for i in range(3):\n",
    "            upsample = np.rot90(upsample, 1, [1, 2])\n",
    "            cp_upsamples_rotated.append(upsample)\n",
    "\n",
    "    cp_upsamples_rotated_flipped = []\n",
    "    for upsample in cp_upsamples_rotated:\n",
    "        cp_upsamples_rotated_flipped.append(upsample)\n",
    "        cp_upsamples_rotated_flipped.append(np.flip(upsample, [2]))\n",
    "\n",
    "    cp_outputs_rotated = []\n",
    "    for output in train_cellprob:\n",
    "        cp_outputs_rotated.append(output)\n",
    "        for i in range(3):\n",
    "            output = np.rot90(output, 1, [0, 1])\n",
    "            cp_outputs_rotated.append(output)\n",
    "\n",
    "    cp_outputs_rotated_flipped = []\n",
    "    for output in cp_outputs_rotated:\n",
    "        cp_outputs_rotated_flipped.append(output)\n",
    "        cp_outputs_rotated_flipped.append(np.flip(output, [1]))\n",
    "\n",
    "    images_cropped = []\n",
    "    for image in images_torch_rotated_flipped:\n",
    "        crops = get_crops(image,256)\n",
    "        for crop in crops:\n",
    "            images_cropped.append(crop)\n",
    "\n",
    "    cp_upsampled_cropped = []\n",
    "    for (i,upsample) in enumerate(cp_upsamples_rotated_flipped):\n",
    "        crops_per_chan = []\n",
    "        for chan in upsample:\n",
    "            crops = get_crops(chan,256)\n",
    "            crops_per_chan.append(crops)\n",
    "\n",
    "        for j in range(16):\n",
    "            crop_channels_of_crop = []\n",
    "            for k in range(32):\n",
    "                crop_channels_of_crop.append(crops_per_chan[k][j])\n",
    "            crop_channels_of_crop = np.array(crop_channels_of_crop)\n",
    "            crop_channels_of_crop = crop_channels_of_crop.copy()\n",
    "            crop_channels_of_crop = torch.from_numpy(crop_channels_of_crop)\n",
    "            cp_upsampled_cropped.append(crop_channels_of_crop)\n",
    "\n",
    "    cp_outputs_cropped = []\n",
    "    for output in cp_outputs_rotated_flipped:\n",
    "        crops = get_crops(output,256)\n",
    "        for crop in crops:\n",
    "            crop = torch.from_numpy(crop.copy())\n",
    "            cp_outputs_cropped.append(crop)\n",
    "\n",
    "    return images_cropped, cp_upsampled_cropped, cp_outputs_cropped, test_images, test_upsamples, test_cellprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KD_loss(torch.nn.Module):\n",
    "    def __init__(self, alpha, beta):\n",
    "        super(KD_loss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def forward(self, y_32_pred, y_32_true, y_3_pred, y_3_true):\n",
    "        \n",
    "        #32-channel loss\n",
    "        y_32_pred = F.sigmoid(y_32_pred)\n",
    "        y_32_true = F.sigmoid(y_32_true)\n",
    "        y_32_loss = F.mse_loss(y_32_pred, y_32_true.float())\n",
    "\n",
    "        #3-channel loss\n",
    "        y_3_bce_loss = F.mse_loss(y_3_pred, y_3_true.float())\n",
    "\n",
    "        return y_32_loss * self.alpha, y_3_bce_loss * self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEpoch(unet, train_loader, test_loader, validation_loader, loss_fn, optimiser, scheduler, epoch_num, device):\n",
    "    time_start = time.time()\n",
    "    \n",
    "    unet.train()\n",
    "\n",
    "    train_y_32_loss, train_map_loss = 0, 0\n",
    "\n",
    "    for image, upsample, cp_output in train_loader:\n",
    "\n",
    "        if device == 'mps':\n",
    "            image, upsample, cp_output = image.float(), upsample.float(), cp_output.float() #cast to float32 (important for mps)\n",
    "\n",
    "        if device is not None:\n",
    "            (image, upsample, cp_output) = (image.to(device),upsample.to(device),cp_output.to(device)) # sending the data to the device (cpu or GPU)\n",
    "\n",
    "        image = image.unsqueeze(1)\n",
    "        y_16_pred, y_32_pred, map_pred = unet(image)\n",
    "        y_32_pred = y_32_pred.squeeze(1)\n",
    "        map_pred = map_pred.squeeze(1)\n",
    "    \n",
    "        loss_32, loss_map = loss_fn(y_32_pred,  upsample, map_pred, cp_output) # calculate the loss of that prediction\n",
    "        train_y_32_loss += loss_32.item()\n",
    "        train_map_loss += loss_map.item()\n",
    "\n",
    "         # zero out the accumulated gradients\n",
    "\n",
    "        #I want to get two losses, one for the 32-channel output and one for the 3-channel output\n",
    "        #I then want to freeze certain channels before putting the losses backwards\n",
    "        unet.encoder.requires_grad = True #repetitive but just to be clear\n",
    "        unet.decoder.requires_grad = True\n",
    "        unet.head.requires_grad = False\n",
    "        loss_32.backward(retain_graph=True)\n",
    "\n",
    "        \n",
    "        unet.encoder.requires_grad = False\n",
    "        unet.decoder.requires_grad = False\n",
    "        unet.head.requires_grad = True\n",
    "        loss_map.backward(retain_graph=True)\n",
    "\n",
    "        optimiser.step() # update model parameters\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    train_y_32_loss, train_map_loss = train_y_32_loss/len(train_loader), train_map_loss/len(train_loader)\n",
    "\n",
    "\n",
    "    val_y_32_loss, val_map_loss, val_IoU = 0, 0, 0\n",
    "    for image, upsample, cp_output in validation_loader:\n",
    "        if device == 'mps':\n",
    "            image, upsample, cp_output = image.float(), upsample.float(), cp_output.float() #cast to float32 (important for mps)\n",
    "\n",
    "        if device is not None:\n",
    "            (image, upsample, cp_output) = (image.to(device),upsample.to(device),cp_output.to(device)) # sending the data to the device (cpu or GPU)\n",
    "\n",
    "        image = image.unsqueeze(1)\n",
    "        y_16_pred, y_32_pred, map_pred = unet(image)\n",
    "        y_32_pred = y_32_pred.squeeze(1)\n",
    "        map_pred = map_pred.squeeze(1)\n",
    "    \n",
    "        loss_32, loss_map = loss_fn(y_32_pred,  upsample, map_pred, cp_output) # calculate the loss of that prediction\n",
    "        val_y_32_loss += loss_32.item()\n",
    "        val_map_loss += loss_map.item()\n",
    "\n",
    "        #IoU score\n",
    "        jaccard = BinaryJaccardIndex(threshold=0.5).to(device)\n",
    "        map_pred = F.sigmoid(map_pred)\n",
    "        cp_output = F.sigmoid(cp_output)\n",
    "        cp_output = torch.where(cp_output > 0.5, 1.0, 0.0)\n",
    "        iou = jaccard(map_pred, cp_output)\n",
    "        if not torch.isnan(iou):\n",
    "            val_IoU += iou\n",
    "        else:\n",
    "            val_IoU += 0\n",
    "        \n",
    "\n",
    "    val_y_32_loss, val_map_loss, val_IoU = val_y_32_loss/len(validation_loader), val_map_loss/len(validation_loader), val_IoU/len(validation_loader)\n",
    "    \n",
    "    #we might add displaying later on\n",
    "    \n",
    "    if epoch_num is None:\n",
    "        print('Train 32 loss: ', train_y_32_loss,'Train map loss', train_map_loss, 'Val 32 loss: ', val_y_32_loss, 'Val map loss: ', val_map_loss, 'Val IoU: ', val_IoU, 'Time: ', time.time()-time_start)\n",
    "    else:\n",
    "        print('Epoch: ', epoch_num, 'Train 32 loss: ', train_y_32_loss,'Train map loss', train_map_loss, 'Val 32 loss: ', val_y_32_loss, 'Val map loss: ', val_map_loss, 'Val IoU: ', val_IoU, 'Time: ', time.time()-time_start)\n",
    "\n",
    "    return unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_directory = \"C:\\\\Users\\\\rz200\\\\Documents\\\\development\\\\distillCellSegTrack\\\\pipeline\\\\uploads\\\\\"\n",
    "#images_directory = \"/Users/rehanzuberi/Documents/Development/distillCellSegTrack/pipeline/uploads\"\n",
    "file_names, images = getImages(images_directory)\n",
    "images_torch = torch.from_numpy(np.array(images))\n",
    "\n",
    "directory = \"C:\\\\Users\\\\rz200\\\\Documents\\\\development\\\\distillCellSegTrack\\\\datasets\\\\Fluo-C2DL-Huh7\\\\01\\\\models\\\\CP_20230601_101328\"\n",
    "#directory = \"/Users/rehanzuberi/Documents/Development/distillCellSegTrack/datasets/Fluo-C2DL-Huh7/01/models/CP_20230601_101328\"\n",
    "cpnet = resnet_torch.CPnet(nbase=[2,32,64,128,256],nout=3,sz=3)\n",
    "cpnet.load_model(directory)\n",
    "\n",
    "images_cropped, cp_upsampled_cropped, cp_outputs_cropped, test_images, test_upsamples, test_cellprob = get_data(images, cpnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, val_images, train_upsamples, val_upsamples, train_cellprob, val_cellprob = train_test_split(images_cropped, cp_upsampled_cropped, cp_outputs_cropped, test_size=0.1, random_state=42)\n",
    "\n",
    "num_train_images = 10\n",
    "train_dataset = ImageDataset(images_cropped, cp_upsampled_cropped, cp_outputs_cropped)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "validation_dataset = ImageDataset(val_images, val_upsamples, val_cellprob)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "test_dataset = ImageDataset(test_images, test_upsamples, test_cellprob)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet(encChannels=(1,32,64,128,256),decChannels=(256,128,64,32),nbClasses=1)\n",
    "unet = unet.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rz200\\AppData\\Local\\anaconda3\\envs\\cellprob\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Train 32 loss:  0.03568900783365178 Train map loss 39.363005401359665 Val 32 loss:  0.035717751862930844 Val map loss:  34.44917507829337 Val IoU:  tensor(0.4493, device='cuda:0') Time:  26.3678081035614\n",
      "Epoch:  1 Train 32 loss:  0.03605732778669335 Train map loss 3.8660736398952498 Val 32 loss:  0.036230350491301765 Val map loss:  1.8937366823826371 Val IoU:  tensor(0.5248, device='cuda:0') Time:  26.915451049804688\n",
      "Epoch:  2 Train 32 loss:  0.036108823493123055 Train map loss 2.0899248885559953 Val 32 loss:  0.03588921384050928 Val map loss:  1.9426379575683126 Val IoU:  tensor(0.4996, device='cuda:0') Time:  26.539201498031616\n",
      "Epoch:  3 Train 32 loss:  0.035301431195370644 Train map loss 2.0159608339890838 Val 32 loss:  0.03462135843161879 Val map loss:  1.8107419907725577 Val IoU:  tensor(0.5190, device='cuda:0') Time:  26.880010843276978\n",
      "Epoch:  4 Train 32 loss:  0.03416552767480931 Train map loss 1.9929214900411252 Val 32 loss:  0.03342814146187799 Val map loss:  1.7534691065164476 Val IoU:  tensor(0.5136, device='cuda:0') Time:  26.85692524909973\n"
     ]
    }
   ],
   "source": [
    "loss_fn = KD_loss(alpha=1, beta=1)\n",
    "optimiser = torch.optim.SGD(unet.parameters(), lr=0.1, momentum=0.1)\n",
    "\n",
    "#I think a decaying scheduler is best\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimiser, base_lr=0.000001, max_lr=0.1)\n",
    "\n",
    "for epoch in range(500):\n",
    "    #print(scheduler.get_last_lr())\n",
    "    unet = trainEpoch(unet, train_loader, test_loader, validation_loader, loss_fn, optimiser, scheduler=scheduler, epoch_num=epoch, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(unet.state_dict(), 'unet_doubletrainfreeze_500epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now need to add post-processing and prediction functions and show some tests data\n",
    "#then will need to add the random seed weight test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun 22 19:23:28 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 472.39       Driver Version: 472.39       CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A4000   WDDM  | 00000000:01:00.0  On |                  Off |\n",
      "| 44%   54C    P2    39W / 140W |   2599MiB / 16376MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1424    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      1780    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      6948    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     20788    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     21736    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     21896      C   ...\\envs\\cellprob\\python.exe    N/A      |\n",
      "|    0   N/A  N/A     24648      C   ...\\envs\\cellprob\\python.exe    N/A      |\n",
      "|    0   N/A  N/A     26148    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A     26824    C+G   ...Anywhere\\AppsAnywhere.exe    N/A      |\n",
      "|    0   N/A  N/A     27780    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     28836    C+G   ...Monitor\\DeviceMonitor.exe    N/A      |\n",
      "|    0   N/A  N/A     32036    C+G   ...ystem32\\UevAppMonitor.exe    N/A      |\n",
      "|    0   N/A  N/A     32320    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A     34296    C+G   ...Box\\Box Edit\\Box Edit.exe    N/A      |\n",
      "|    0   N/A  N/A     35420    C+G   ...oft OneDrive\\OneDrive.exe    N/A      |\n",
      "|    0   N/A  N/A     35700    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A     39424    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     39808    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     44132    C+G   ...ge\\Application\\msedge.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_IoU = 0\n",
    "for image, upsample, cp_output in validation_loader:\n",
    "\n",
    "    (image, upsample, cp_output) = (image.to('cuda:0'),upsample.to('cuda:0'),cp_output.to('cuda:0')) # sending the data to the device (cpu or GPU)\n",
    "\n",
    "    image = image.unsqueeze(1)\n",
    "    y_16_pred, y_32_pred, map_pred = unet(image)\n",
    "    y_32_pred = y_32_pred.squeeze(1)\n",
    "    map_pred = map_pred.squeeze(1)\n",
    "\n",
    "    device = 'cuda:0'\n",
    "    #IoU score\n",
    "    jaccard = BinaryJaccardIndex(threshold=0.5).to(device)\n",
    "    map_pred = F.sigmoid(map_pred)\n",
    "    cp_output = F.sigmoid(cp_output)\n",
    "    cp_output = torch.where(cp_output > 0.5, 1.0, 0.0)\n",
    "    iou = jaccard(map_pred, cp_output)\n",
    "    if not torch.isnan(iou):\n",
    "        val_IoU += iou\n",
    "    else:\n",
    "        val_IoU += 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cp_output = cp_output.squeeze(0)\n",
    "cp_output = cp_output.cpu().detach().numpy()\n",
    "plt.imshow(cp_output)\n",
    "print(np.unique(cp_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(59.5064, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(val_IoU)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
